{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb2608b-29e3-43eb-b94a-0f4a4618e742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- One-Hot Encoding ---\n",
      "   about  accurately  across  actively  ad  addressed  addressing  \\\n",
      "0      0           0       0         1   0          1           0   \n",
      "1      0           1       1         0   1          0           0   \n",
      "2      1           0       0         0   0          0           1   \n",
      "\n",
      "   adversarial  ai  also  ...  which  while  widely  will  with  without  \\\n",
      "0            1   1     1  ...      1      0       1     0     1        0   \n",
      "1            0   1     1  ...      0      0       0     1     0        0   \n",
      "2            0   1     1  ...      0      1       0     0     1        1   \n",
      "\n",
      "   work  write  written  years  \n",
      "0     0      1        0      1  \n",
      "1     0      1        0      0  \n",
      "2     1      0        1      0  \n",
      "\n",
      "[3 rows x 306 columns]\n"
     ]
    }
   ],
   "source": [
    "# 24. Write a program to read a 3 text files on any technical concept with at least\n",
    "#20 sentences and 150 words. Implement one-hot encoding. \n",
    "# ONE-HOT ENCODING\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Read documents from the files\n",
    "filenames = [r\"D:\\College\\AIL\\file1.txt\", r\"D:\\College\\AIL\\file2.txt\", r\"D:\\College\\AIL\\file3.txt\"]  # Replace with your actual file paths\n",
    "documents = [open(file, \"r\", encoding=\"utf-8\").read() for file in filenames]\n",
    "\n",
    "# Implementing One-Hot Encoding (binary count)\n",
    "vectorizer_oh = CountVectorizer(binary=True)  # binary=True for One-Hot Encoding\n",
    "onehot = vectorizer_oh.fit_transform(documents)\n",
    "\n",
    "# Convert the One-Hot encoded data into a DataFrame for easy viewing\n",
    "df_oh = pd.DataFrame(onehot.toarray(), columns=vectorizer_oh.get_feature_names_out())\n",
    "\n",
    "# Display the result\n",
    "print(\"\\n--- One-Hot Encoding ---\")\n",
    "print(df_oh.head())  # Display the first few rows of the One-Hot encoded DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c113f0-dce0-42dc-99de-5351c5c4a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bag of Words ---\n",
      "   about  accurately  across  actively  ad  addressed  addressing  \\\n",
      "0      0           0       0         1   0          1           0   \n",
      "1      0           1       1         0   1          0           0   \n",
      "2      2           0       0         0   0          0           1   \n",
      "\n",
      "   adversarial  ai  also  ...  which  while  widely  will  with  without  \\\n",
      "0            1  10     2  ...      1      0       1     0     1        0   \n",
      "1            0   9     1  ...      0      0       0     1     0        0   \n",
      "2            0   7     3  ...      0      1       0     0     1        1   \n",
      "\n",
      "   work  write  written  years  \n",
      "0     0      1        0      1  \n",
      "1     0      1        0      0  \n",
      "2     1      0        1      0  \n",
      "\n",
      "[3 rows x 306 columns]\n"
     ]
    }
   ],
   "source": [
    "#25. Write a program to read a 3 text files on a movie review with at least 20\n",
    "#sentences and 150 words. Implement bag of words.\n",
    "# BAG OF WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Corrected file paths using raw string literals\n",
    "filenames = [r\"D:\\College\\AIL\\file1.txt\", r\"D:\\College\\AIL\\file2.txt\", r\"D:\\College\\AIL\\file3.txt\"]\n",
    "\n",
    "# Read documents from the files\n",
    "documents = [open(file, \"r\", encoding=\"utf-8\").read() for file in filenames]\n",
    "\n",
    "# Implementing Bag of Words\n",
    "vectorizer_bow = CountVectorizer()  # Bag of Words without binary encoding\n",
    "bow = vectorizer_bow.fit_transform(documents)\n",
    "\n",
    "# Convert the Bag of Words data into a DataFrame for easy viewing\n",
    "df_bow = pd.DataFrame(bow.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
    "\n",
    "# Display the result\n",
    "print(\"\\n--- Bag of Words ---\")\n",
    "print(df_bow.head())  # Display the first few rows of the Bag of Words DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c1bdc5-cccc-425f-bd3a-94cf0c317681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: D:\\College\\AIL\\file1.txt\n",
      "Reading file: D:\\College\\AIL\\file2.txt\n",
      "Reading file: D:\\College\\AIL\\file3.txt\n",
      "\n",
      "--- TF-IDF ---\n",
      "      about  accurately    across  actively        ad  addressed  addressing  \\\n",
      "0  0.000000    0.000000  0.000000  0.061142  0.000000   0.061142    0.000000   \n",
      "1  0.000000    0.062571  0.062571  0.000000  0.062571   0.000000    0.000000   \n",
      "2  0.125568    0.000000  0.000000  0.000000  0.000000   0.000000    0.062784   \n",
      "\n",
      "   adversarial        ai      also  ...     which     while    widely  \\\n",
      "0     0.061142  0.361112  0.072222  ...  0.061142  0.000000  0.061142   \n",
      "1     0.000000  0.332601  0.036956  ...  0.000000  0.000000  0.000000   \n",
      "2     0.000000  0.259569  0.111244  ...  0.000000  0.062784  0.000000   \n",
      "\n",
      "       will      with   without      work     write   written     years  \n",
      "0  0.000000  0.046500  0.000000  0.000000  0.046500  0.000000  0.061142  \n",
      "1  0.062571  0.000000  0.000000  0.000000  0.047587  0.000000  0.000000  \n",
      "2  0.000000  0.047749  0.062784  0.062784  0.000000  0.062784  0.000000  \n",
      "\n",
      "[3 rows x 306 columns]\n"
     ]
    }
   ],
   "source": [
    "# 26. Write a program to read a 3 text files a tourist place with at least 20\n",
    "# sentences and 150 words. Implement TF-IDF. \n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Corrected file paths using raw string literals\n",
    "filenames = [r\"D:\\College\\AIL\\file1.txt\", r\"D:\\College\\AIL\\file2.txt\", r\"D:\\College\\AIL\\file3.txt\"]\n",
    "\n",
    "# Check if the files exist before trying to read them\n",
    "for file in filenames:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"File not found: {file}\")\n",
    "    else:\n",
    "        print(f\"Reading file: {file}\")\n",
    "\n",
    "# Read documents from the files\n",
    "documents = [open(file, \"r\", encoding=\"utf-8\").read() for file in filenames if os.path.exists(file)]\n",
    "\n",
    "# Implementing TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()  # TF-IDF vectorizer\n",
    "tfidf = vectorizer_tfidf.fit_transform(documents)\n",
    "\n",
    "# Convert the TF-IDF data into a DataFrame for easy viewing\n",
    "df_tfidf = pd.DataFrame(tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "\n",
    "# Display the result\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "print(df_tfidf.head())  # Display the first few rows of the TF-IDF DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d2edf-7cf9-474a-8be7-c8a6095a69f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
